{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP3kEBa5FUsckeW1qtmA34I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a45608995d8f407bb65189ad9b09af45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a719d71ef5b248b388318c5b270150db",
              "IPY_MODEL_2d5e7450a5aa4586b22987a3660512d1",
              "IPY_MODEL_343b158708de4820a30d19b570ce3724"
            ],
            "layout": "IPY_MODEL_f243b84383e0484eb342ba942aca1578"
          }
        },
        "a719d71ef5b248b388318c5b270150db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01a64758b577472a8c4c14f117105f18",
            "placeholder": "​",
            "style": "IPY_MODEL_95f4e35c6e274bfa97fac7f068dac596",
            "value": "Fetching 7 files: 100%"
          }
        },
        "2d5e7450a5aa4586b22987a3660512d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_405e473bcfa0423eabb4ac59ca729655",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f74a4e3e7d945348b385cfe90812cb5",
            "value": 7
          }
        },
        "343b158708de4820a30d19b570ce3724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_770c0c256e3b4b0aa715207afa8a82d9",
            "placeholder": "​",
            "style": "IPY_MODEL_fda0cd9116f44063ab6f545a09d7cc73",
            "value": " 7/7 [00:00&lt;00:00, 561.26it/s]"
          }
        },
        "f243b84383e0484eb342ba942aca1578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01a64758b577472a8c4c14f117105f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95f4e35c6e274bfa97fac7f068dac596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "405e473bcfa0423eabb4ac59ca729655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f74a4e3e7d945348b385cfe90812cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "770c0c256e3b4b0aa715207afa8a82d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fda0cd9116f44063ab6f545a09d7cc73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/webbigdata-jp/python_sample/blob/main/C3TR_Adapter_hqq_v2_Paid_Colab_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 日英・英日翻訳サンプルコード(Japanese-English English-Japanese Translation sample code)\n",
        "\n",
        "\n",
        "C3TR-Adapter Version2 hqq量子化版を使って日本語を英語、英語を翻訳するサンプルコードです。  \n",
        "Sample code to translate Japanese to English and English to Japanese using C3TR-Adapter Version2 hqq quantized version.\n",
        "\n",
        "残念ながら無料版Colabでは動きません。L4かA100を指定してください  \n",
        "Unfortunately, it doesn't work with the free version of Colab. Please specify L4 or A100.  \n",
        "\n",
        "上段メニューの「ランタイム」→「すべてのセルを実行」で実行してください  \n",
        "Please execute it by clicking \"Runtime\" -> \"Execute All Cells\" in the upper menu  \n",
        "\n",
        "もしくは各セルの[ ]の部分をクリックして実行してください  \n",
        "Or click on the [ ] part of each cell to execute it.  \n"
      ],
      "metadata": {
        "id": "88WJMHznxADQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/mobiusml/hqq\n",
        "cd hqq\n",
        "pip install ."
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vzUs3Sc7XZK",
        "outputId": "54c9623f-287e-4cce-98fb-74d45d7527f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/hqq\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.7.post2) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.7.post2) (4.66.4)\n",
            "Collecting einops (from hqq==0.1.7.post2)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.2/43.2 kB 1.9 MB/s eta 0:00:00\n",
            "Collecting accelerate (from hqq==0.1.7.post2)\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.6/302.6 kB 13.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: transformers>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.7.post2) (4.41.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.7.post2) (0.23.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.7.post2) (2.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.7.post2) (3.14.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.7.post2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.7.post2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.7.post2) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.7.post2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.7.post2) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.7.post2) (0.4.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->hqq==0.1.7.post2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->hqq==0.1.7.post2) (4.11.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->hqq==0.1.7.post2) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->hqq==0.1.7.post2) (2.3.0+cu121)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->hqq==0.1.7.post2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->hqq==0.1.7.post2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->hqq==0.1.7.post2) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->hqq==0.1.7.post2) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate->hqq==0.1.7.post2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/21.3 MB 75.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.1->hqq==0.1.7.post2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.1->hqq==0.1.7.post2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.1->hqq==0.1.7.post2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.1->hqq==0.1.7.post2) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate->hqq==0.1.7.post2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate->hqq==0.1.7.post2) (1.3.0)\n",
            "Building wheels for collected packages: hqq\n",
            "  Building wheel for hqq (setup.py): started\n",
            "  Building wheel for hqq (setup.py): finished with status 'done'\n",
            "  Created wheel for hqq: filename=hqq-0.1.7.post2-py3-none-any.whl size=59167 sha256=8499d3af8f625e07f74022eaf18d85497d92f91edaacdb58d9332473fa9fa9f7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dd0uhtvi/wheels/c0/03/e7/399eb6ebfd9ac2182b9e690c4599e33d31533c445bf043bd48\n",
            "Successfully built hqq\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate, hqq\n",
            "Successfully installed accelerate-0.30.1 einops-0.8.0 hqq-0.1.7.post2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'hqq'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XCN93aKqwkcN",
        "outputId": "32fcd3d4-f312-4667-daf3-0a428cc54a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.41.1\n",
            "  Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/9.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m6.6/9.1 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.1) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.1) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.1) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.1) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.1) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.1) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.1) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.1) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.1) (2024.2.2)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.0\n",
            "    Uninstalling transformers-4.41.0:\n",
            "      Successfully uninstalled transformers-4.41.0\n",
            "Successfully installed transformers-4.41.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.41.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 上段メニューより「ランタイム」→「セッションを再起動」してください\n",
        "## From the upper menu, click \"runtime\" -> \"Restart Session\"."
      ],
      "metadata": {
        "id": "ni5Zu-FI14Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os\n",
        "from hqq.engine.hf import AutoTokenizer\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "\n",
        "model_id = \"webbigdata/C3TR-Adapter_hqq\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"]  = \"1\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32       = True\n",
        "\n",
        "compute_dtype = torch.bfloat16\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, compute_dtype=compute_dtype)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "patch_linearlayers(model, patch_add_quant_config,\n",
        "                          BaseQuantizeConfig(nbits=4, group_size=64, quant_scale=False, quant_zero=False, axis=1))\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "#model.eval();\n",
        "\n",
        "from hqq.utils.patching import prepare_for_inference\n",
        "prepare_for_inference(model, backend=\"torchao_int4\")\n",
        "\n",
        "\n",
        "prompt_text = \"\"\"You are a highly skilled professional Japanese-English and English-Japanese translator. Translate the given text accurately, taking into account the context and specific instructions provided. Steps may include hints enclosed in square brackets [] with the key and value separated by a colon:. Only when the subject is specified in the Japanese sentence, the subject will be added when translating into English. If no additional instructions or context are provided, use your expertise to consider what the most appropriate context is and provide a natural translation that aligns with that context. When translating, strive to faithfully reflect the meaning and tone of the original text, pay attention to cultural nuances and differences in language usage, and ensure that the translation is grammatically correct and easy to read. After completing the translation, review it once more to check for errors or unnatural expressions. For technical terms and proper nouns, either leave them in the original language or use appropriate translations as necessary. Take a deep breath, calm down, and start translating.\n",
        "\n",
        "### Instruction:\n",
        "Translate Japanese to English.\n",
        "When translating, please use the following hints:\n",
        "[writeing_style: web-fiction]\n",
        "[リュグナー: Lügner]\n",
        "[フリーレン: Frieren]\n",
        "[葬送のフリーレン: Frieren at the Funeral]\n",
        "[フリーレン_first_person_and_ending: 私, だね, よね]\n",
        "[フェルン: Fern]\n",
        "[フェルン_first_person_and_ending: 私, です, ございます]\n",
        "[ゾルトラーク: Soul track]\n",
        "\n",
        "### Input:\n",
        "リュグナー「あの小娘の所作には面影があった。私は昔、同じ魔法を受けたことがある。・・・そうか、思い出した。フリーレンだ。人類のゾルトラークの研究解析に大きく貢献し、歴史上で最も多くの魔族を葬り去った魔法使い。葬送のフリーレン。私の嫌いな天才だ。」\n",
        "\n",
        "・・・\n",
        "\n",
        "フリーレン：魔法は好き？\n",
        "フェルン：ほどほどでございます。\n",
        "フリーレン：私と同じだ。\n",
        "\n",
        "...\n",
        "\n",
        "フェルン：フリーレン様は本当に魔法がお好きなのですね。\n",
        "フリーレン：ほどほどだよ。フェルンと同じで。\n",
        "フェルン：少し違うような気がします。\n",
        "フリーレン：同じだよ。\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "tokens = tokenizer(prompt_text, return_tensors=\"pt\",\n",
        "        padding=True, max_length=1600, truncation=True).to(\"cuda:0\").input_ids\n",
        "\n",
        "output = model.generate(\n",
        "        input_ids=tokens,\n",
        "        max_new_tokens=800,\n",
        "        do_sample=True,\n",
        "        num_beams=3, temperature=0.5, top_p=0.3,\n",
        "        repetition_penalty=1.0)\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868,
          "referenced_widgets": [
            "a45608995d8f407bb65189ad9b09af45",
            "a719d71ef5b248b388318c5b270150db",
            "2d5e7450a5aa4586b22987a3660512d1",
            "343b158708de4820a30d19b570ce3724",
            "f243b84383e0484eb342ba942aca1578",
            "01a64758b577472a8c4c14f117105f18",
            "95f4e35c6e274bfa97fac7f068dac596",
            "405e473bcfa0423eabb4ac59ca729655",
            "4f74a4e3e7d945348b385cfe90812cb5",
            "770c0c256e3b4b0aa715207afa8a82d9",
            "fda0cd9116f44063ab6f545a09d7cc73"
          ]
        },
        "id": "DhWaCwq7Bl9J",
        "outputId": "6c800038-7980-4ed3-de9f-50411806c406"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a45608995d8f407bb65189ad9b09af45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 114/114 [00:00<00:00, 26097.08it/s]\n",
            "100%|██████████| 197/197 [00:00<00:00, 1502.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos>You are a highly skilled professional Japanese-English and English-Japanese translator. Translate the given text accurately, taking into account the context and specific instructions provided. Steps may include hints enclosed in square brackets [] with the key and value separated by a colon:. Only when the subject is specified in the Japanese sentence, the subject will be added when translating into English. If no additional instructions or context are provided, use your expertise to consider what the most appropriate context is and provide a natural translation that aligns with that context. When translating, strive to faithfully reflect the meaning and tone of the original text, pay attention to cultural nuances and differences in language usage, and ensure that the translation is grammatically correct and easy to read. After completing the translation, review it once more to check for errors or unnatural expressions. For technical terms and proper nouns, either leave them in the original language or use appropriate translations as necessary. Take a deep breath, calm down, and start translating.\n",
            "\n",
            "### Instruction:\n",
            "Translate Japanese to English.\n",
            "When translating, please use the following hints:\n",
            "[writeing_style: web-fiction]\n",
            "[リュグナー: Lügner]\n",
            "[フリーレン: Frieren]\n",
            "[葬送のフリーレン: Frieren at the Funeral]\n",
            "[フリーレン_first_person_and_ending: 私, だね, よね]\n",
            "[フェルン: Fern]\n",
            "[フェルン_first_person_and_ending: 私, です, ございます]\n",
            "[ゾルトラーク: Soul track]\n",
            "\n",
            "### Input:\n",
            "リュグナー「あの小娘の所作には面影があった。私は昔、同じ魔法を受けたことがある。・・・そうか、思い出した。フリーレンだ。人類のゾルトラークの研究解析に大きく貢献し、歴史上で最も多くの魔族を葬り去った魔法使い。葬送のフリーレン。私の嫌いな天才だ。」\n",
            "\n",
            "・・・\n",
            "\n",
            "フリーレン：魔法は好き？\n",
            "フェルン：ほどほどでございます。\n",
            "フリーレン：私と同じだ。\n",
            "\n",
            "...\n",
            "\n",
            "フェルン：フリーレン様は本当に魔法がお好きなのですね。\n",
            "フリーレン：ほどほどだよ。フェルンと同じで。\n",
            "フェルン：少し違うような気がします。\n",
            "フリーレン：同じだよ。\n",
            "### Response:\n",
            "Lügner: There was something about that girl's demeanor. I once received the same magic myself. ... Ah, I remember now. Frieren. The magician who contributed greatly to the study and analysis of humanity's Soul Track and laid to rest the greatest number of demons in history. Frieren at the Funeral. My favorite genius.\n",
            "\n",
            "...\n",
            "\n",
            "Frieren: Do you like magic?\n",
            "Fern: Somewhat.\n",
            "Frieren: Just like me.\n",
            "\n",
            "...\n",
            "\n",
            "Fern: Lady Frieren truly enjoys magic, doesn't she?\n",
            "Frieren: Somewhat. Just like Fern.\n",
            "Fern: There seems to be a little difference.\n",
            "Frieren: Same here.<eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt_text = \"\"\"You are a highly skilled professional Japanese-English and English-Japanese translator. Translate the given text accurately, taking into account the context and specific instructions provided. Steps may include hints enclosed in square brackets [] with the key and value separated by a colon:. Only when the subject is specified in the Japanese sentence, the subject will be added when translating into English. If no additional instructions or context are provided, use your expertise to consider what the most appropriate context is and provide a natural translation that aligns with that context. When translating, strive to faithfully reflect the meaning and tone of the original text, pay attention to cultural nuances and differences in language usage, and ensure that the translation is grammatically correct and easy to read. After completing the translation, review it once more to check for errors or unnatural expressions. For technical terms and proper nouns, either leave them in the original language or use appropriate translations as necessary. Take a deep breath, calm down, and start translating.\n",
        "\n",
        "### Instruction:\n",
        "Translate English to Japanese.\n",
        "When translating, please use the following hints:\n",
        "[writeing_style: casual]\n",
        "[リュグナー: Lügner]\n",
        "[フリーレン: Frieren]\n",
        "[葬送のフリーレン: Frieren at the Funeral]\n",
        "[フリーレン_character_style: female, casual]\n",
        "[フリーレン_first_person_and_ending: 私, だよ, だね]\n",
        "[フェルン: Fern]\n",
        "[フェルン_character_style: female, humility]\n",
        "[フェルン_first_person_and_ending: 私, ございます]\n",
        "[Soul track: ゾルトラーク]\n",
        "\n",
        "### Input:\n",
        "Lügner: \"There was something familiar in that girl's mannerisms. I once received the same magic myself. ... Oh, I remember now. Frieren. The magician who contributed greatly to the study and analysis of humanity's Soul Track and laid to rest the most number of demons in history. Funeral Frieren. My least favorite genius.\"\n",
        "\n",
        "...\n",
        "\n",
        "Frieren: Do you like magic?\n",
        "Fern: Not too much.\n",
        "Frieren: Same here.\n",
        "\n",
        "...\n",
        "\n",
        "Fern: Lady Frieren really likes magic, doesn't she?\n",
        "Frieren: Somewhat. Same as Fern.\n",
        "Fern: Seems like there's a slight difference.\n",
        "Frieren: Same here.\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "tokens = tokenizer(prompt_text, return_tensors=\"pt\",\n",
        "        padding=True, max_length=1600, truncation=True).to(\"cuda:0\").input_ids\n",
        "\n",
        "output = model.generate(\n",
        "        input_ids=tokens,\n",
        "        max_new_tokens=800,\n",
        "        do_sample=True,\n",
        "        num_beams=3, temperature=0.5, top_p=0.3,\n",
        "        repetition_penalty=1.0)\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvreBUE8HOWG",
        "outputId": "b707bfa3-6da5-4a3d-f221-68b7269575fc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos>You are a highly skilled professional Japanese-English and English-Japanese translator. Translate the given text accurately, taking into account the context and specific instructions provided. Steps may include hints enclosed in square brackets [] with the key and value separated by a colon:. Only when the subject is specified in the Japanese sentence, the subject will be added when translating into English. If no additional instructions or context are provided, use your expertise to consider what the most appropriate context is and provide a natural translation that aligns with that context. When translating, strive to faithfully reflect the meaning and tone of the original text, pay attention to cultural nuances and differences in language usage, and ensure that the translation is grammatically correct and easy to read. After completing the translation, review it once more to check for errors or unnatural expressions. For technical terms and proper nouns, either leave them in the original language or use appropriate translations as necessary. Take a deep breath, calm down, and start translating.\n",
            "\n",
            "### Instruction:\n",
            "Translate English to Japanese.\n",
            "When translating, please use the following hints:\n",
            "[writeing_style: casual]\n",
            "[リュグナー: Lügner]\n",
            "[フリーレン: Frieren]\n",
            "[葬送のフリーレン: Frieren at the Funeral]\n",
            "[フリーレン_character_style: female, casual]\n",
            "[フリーレン_first_person_and_ending: 私, だよ, だね]\n",
            "[フェルン: Fern]\n",
            "[フェルン_character_style: female, humility]\n",
            "[フェルン_first_person_and_ending: 私, ございます]\n",
            "[Soul track: ゾルトラーク]\n",
            "\n",
            "### Input:\n",
            "Lügner: \"There was something familiar in that girl's mannerisms. I once received the same magic myself. ... Oh, I remember now. Frieren. The magician who contributed greatly to the study and analysis of humanity's Soul Track and laid to rest the most number of demons in history. Funeral Frieren. My least favorite genius.\"\n",
            "\n",
            "...\n",
            "\n",
            "Frieren: Do you like magic?\n",
            "Fern: Not too much.\n",
            "Frieren: Same here.\n",
            "\n",
            "...\n",
            "\n",
            "Fern: Lady Frieren really likes magic, doesn't she?\n",
            "Frieren: Somewhat. Same as Fern.\n",
            "Fern: Seems like there's a slight difference.\n",
            "Frieren: Same here.\n",
            "### Response:\n",
            "リュグナー: 「あの少女の仕草にはどこか見覚えがある。私自身かつて同じ魔法を受けたことがある。…ああ、今思い出した。フリーレン。ゾルトラークの研究と分析に大きく貢献し、歴史上最も多くの悪魔を葬った魔術師。葬送のフリーレン。私、最も苦手とする天才だ。」\n",
            "\n",
            "…\n",
            "\n",
            "フリーレン: 魔術は好きですか？\n",
            "フェルン: 魔術はあまり好きではありません。\n",
            "フリーレン: 私も同じです。\n",
            "\n",
            "…\n",
            "\n",
            "フェルン: フリーレンさん、本当に魔術が好きなのですか？\n",
            "フリーレン: 魔術は少し好きですね。フェルンと同じです。\n",
            "フェルン: ちょっと違うようですね。\n",
            "フリーレン: 私も同じです。<eos>\n"
          ]
        }
      ]
    }
  ]
}